model:
  name: "lecslab/glosslm"
  max_length: 512

data:
  hf_uri: "tira-parsing/fst-output"
  columns:
    input_column: "orig_text"
    target_column: "FST_text"
  task:
    prompt: |
      prompt = f"""Provide the glosses for the following transcription in {lang}.

      Transcription in {lang}: {transcription}
      Transcription segmented: False
      Translation in {metalang}: {translation}

      Glosses: 

training:
  output_dir: "./outputs"
  learning_rate: 3e-4
  batch_size: 4
  grad_acc: 8
  epochs: 10
  bf16: true # Set to false if not on Ampere+ GPU
  save_total_limit: 2

wandb:
  project: "Tira-LM"
  run_name: "byt5-base-pretrain"